{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Census Tract Segmentation Analysis\n",
    "\n",
    "This script analyzes the integrated tract data to find patterns and groupings\n",
    "based on demographic and socioeconomic characteristics, without using\n",
    "geographical information during clustering. After clustering, geographic\n",
    "information is reintroduced to identify \"doppelganger\" tracts (similar\n",
    "characteristics in different locations).\n",
    "\n",
    "The script implements:\n",
    "    1. Advanced data loading and preprocessing\n",
    "    2. Statistical analysis and feature selection\n",
    "    3. Dimensionality reduction (PCA, UMAP)\n",
    "    4. Cluster analysis (KMeans, GaussianMixture, HDBSCAN)\n",
    "    5. XGBoost classification with hyperparameter optimization\n",
    "    6. Feature importance analysis\n",
    "    7. Geographic doppelganger identification (sampling, NearestNeighbors)\n",
    "\n",
    "Author: Jacob\n",
    "Date: April 26, 2025\n",
    "Version: 3.7.1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "import platform\n",
    "import subprocess\n",
    "from time import time\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any, Set, cast\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b4e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ae8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar imports\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    from tqdm.notebook import tqdm as tqdm_notebook\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    # Define fallback if tqdm is not available\n",
    "    print(\"tqdm not available, installing...\")\n",
    "    try:\n",
    "        import pip\n",
    "        pip.main(['install', 'tqdm'])\n",
    "        from tqdm import tqdm\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        TQDM_AVAILABLE = True\n",
    "    except:\n",
    "        print(\"Error installing tqdm. Progress bars will not be available.\")\n",
    "        # Define a simple fallback tqdm function\n",
    "        def tqdm(iterable, **kwargs):\n",
    "            return iterable\n",
    "        tqdm_notebook = tqdm\n",
    "        TQDM_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ddd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning imports\n",
    "# - Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "try:\n",
    "    import hdbscan  # type: ignore\n",
    "except ImportError:\n",
    "    hdbscan = None\n",
    "try:\n",
    "    import umap  # type: ignore\n",
    "except ImportError:\n",
    "    umap = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32dd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Classification and feature selection\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd090e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Metrics and model selection\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, \n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb479ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Outlier detection\n",
    "from sklearn.covariance import EllipticEnvelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and constants\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "# Detect if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules or os.environ.get('IN_COLAB') == '1' or '--colab' in sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6adb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Set your base directory as you mounted it in Google Drive\n",
    "    BASE_DIR = \"/content/drive/Othercomputers/My Laptop/Project #2 (Weeks 5 thru 8)/Week #5\"\n",
    "else:\n",
    "    # Local Windows path\n",
    "    BASE_DIR = r\"C:\\Users\\jacob\\DSC680 - Applied Data Science\\Project #2 (Weeks 5 thru 8)\\Week #5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = os.path.join(BASE_DIR, \"output\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"output\", \"segmentation\")\n",
    "INPUT_FILE: str = 'integrated_tract_data_2022.gpkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9148f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU/CUDA Configuration\n",
    "GPU_ENABLED: bool = False  # Will be set dynamically\n",
    "CUDA_AVAILABLE: bool = False  # Will be set dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar configuration\n",
    "USE_PROGRESS_BARS: bool = True  # Set to False to disable all progress bars\n",
    "IS_NOTEBOOK: bool = 'ipykernel' in sys.modules  # Auto-detect if running in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e69055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis parameters\n",
    "MISSING_THRESHOLD: float = 0.8  # Drop columns with more than 80% missing values\n",
    "LOW_VAR_THRESHOLD: float = 0.01  # Threshold for low variance columns\n",
    "EXTREME_CORR_THRESHOLD: float = 0.95  # Threshold for extreme correlations\n",
    "OUTLIER_IQR_FACTOR: float = 1.5  # Factor for IQR-based outlier detection\n",
    "EXTREME_OUTLIER_THRESHOLD: float = -0.5  # Threshold for extreme outliers in Isolation Forest\n",
    "MIN_CLUSTERS: int = 2  # Minimum number of clusters\n",
    "MAX_CLUSTERS: int = 15  # Maximum number of clusters\n",
    "# Default cluster counts\n",
    "CLUSTER_COUNTS: List[int] = [10, 11, 12]  # Default cluster counts to evaluate\n",
    "DEFAULT_CLUSTER_COUNT: int = CLUSTER_COUNTS[0]  # Primary cluster count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fdc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doppelganger evaluation parameters\n",
    "DOPPELGANGER_SAMPLE_SIZE: int = 10  # Number of doppelganger pairs per state\n",
    "DOPPELGANGER_SIMILARITY_THRESHOLD: float = 0.0  # Minimum similarity to include\n",
    "SIMILARITY_METRIC: str = \"cosine\"  # Metric for similarity calculation\n",
    "FEATURE_WEIGHT_DEFAULT: float = 1.0  # Default weight for all features\n",
    "FEATURE_WEIGHTS: Dict[str, float] = {}  # Feature-specific weights; defaults to FEATURE_WEIGHT_DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc094e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## Algorithm options (comment out to disable)\n",
    "CLUSTER_ALGOS: List[str] = [\"kmeans\", \"gmm\", \"hdbscan\"]  # Clustering methods\n",
    "DR_METHODS: List[str] = [\"pca\", \"umap\"]  # Dimensionality reduction methods\n",
    "DOPPELGANGER_METHODS: List[str] = [\"sampling\", \"nearest_neighbors\"]  # Doppelganger search methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcac36f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Check for GPU/CUDA availability\n",
    "def check_gpu_availability() -> Tuple[bool, bool]:\n",
    "    \"\"\"Check if GPU and CUDA are available for computation acceleration.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[bool, bool]: (gpu_enabled, cuda_available)\n",
    "    \"\"\"\n",
    "    gpu_enabled = False\n",
    "    cuda_available = False\n",
    "    \n",
    "    try:\n",
    "        # Check for NVIDIA GPU using system-specific methods\n",
    "        if platform.system() == \"Windows\":\n",
    "            # On Windows, use nvidia-smi\n",
    "            try:\n",
    "                nvidia_smi_output = subprocess.check_output([\"nvidia-smi\"], \n",
    "                                     stderr=subprocess.DEVNULL,\n",
    "                                     universal_newlines=True)\n",
    "                gpu_enabled = True\n",
    "                if \"CUDA Version\" in nvidia_smi_output:\n",
    "                    cuda_available = True\n",
    "            except (subprocess.SubprocessError, FileNotFoundError):\n",
    "                pass\n",
    "        \n",
    "        # Also try checking within Python using available packages\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_enabled = True\n",
    "                cuda_available = True\n",
    "        except ImportError:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            if tf.test.is_gpu_available(cuda_only=True):\n",
    "                gpu_enabled = True\n",
    "                cuda_available = True\n",
    "        except (ImportError, AttributeError):\n",
    "            # TF 2.x uses different API\n",
    "            try:\n",
    "                import tensorflow as tf\n",
    "                if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "                    gpu_enabled = True\n",
    "                    cuda_available = True\n",
    "            except (ImportError, AttributeError):\n",
    "                pass\n",
    "                \n",
    "        # Check via XGBoost itself\n",
    "        try:\n",
    "            import xgboost as xgb\n",
    "            from xgboost import XGBClassifier\n",
    "            # Try creating a small model with GPU parameters\n",
    "            try:\n",
    "                temp_model = XGBClassifier(tree_method='gpu_hist', n_estimators=1)\n",
    "                temp_model.fit(np.array([[0, 0], [1, 1]]), np.array([0, 1]))\n",
    "                gpu_enabled = True\n",
    "                cuda_available = True\n",
    "            except Exception:\n",
    "                pass\n",
    "        except ImportError:\n",
    "            pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error checking GPU availability: {e}\")\n",
    "    \n",
    "    return gpu_enabled, cuda_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU flags\n",
    "GPU_ENABLED, CUDA_AVAILABLE = check_gpu_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dfd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10764b96",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"GPU Enabled: {GPU_ENABLED}\")\n",
    "print(f\"CUDA Available: {CUDA_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73232da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractSegmentationAnalysis:\n",
    "    \"\"\"Class for analyzing and segmenting census tract data.\n",
    "    \n",
    "    This class provides methods for loading, preprocessing, and analyzing census tract data\n",
    "    to identify meaningful clusters of similar areas based on socioeconomic and demographic\n",
    "    characteristics. It employs advanced techniques for data cleaning, feature selection,\n",
    "    dimensionality reduction, and machine learning to generate high-quality segmentation.\n",
    "    \n",
    "    Attributes:\n",
    "        input_path (str): Directory path where input data is located\n",
    "        input_file (str): Filename of the input geopackage file\n",
    "        output_path (str): Directory path where analysis outputs will be saved\n",
    "        data (pd.DataFrame): Original loaded data (without preprocessing)\n",
    "        geo_data (gpd.GeoDataFrame): Original data with geometry information\n",
    "        numeric_data (pd.DataFrame): Processed data for analysis (numeric features only)\n",
    "        id_data (pd.DataFrame): Identifier columns from the original dataset\n",
    "        cluster_results (pd.DataFrame): Original data with cluster assignments\n",
    "        pca_results (pd.DataFrame): Results of PCA dimensionality reduction\n",
    "        feature_importance (pd.DataFrame): Feature importance rankings\n",
    "        best_model (XGBClassifier): Best XGBoost classifier from hyperparameter tuning\n",
    "        best_params (dict): Best hyperparameters from grid search\n",
    "        cluster_labels (np.ndarray): Cluster assignments for each observation\n",
    "        evaluation_metrics (dict): Performance metrics for the classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_path: str = INPUT_PATH, \n",
    "             input_file: str = INPUT_FILE, \n",
    "             output_path: str = OUTPUT_PATH,\n",
    "             use_gpu: bool = GPU_ENABLED) -> None:\n",
    "        \"\"\"Initialize the TractSegmentationAnalysis class.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Directory path containing the input data file\n",
    "            input_file: Name of the geopackage file to analyze\n",
    "            output_path: Directory path where output files will be saved\n",
    "            use_gpu: Whether to use GPU acceleration if available\n",
    "        \"\"\"\n",
    "        self.input_path: str = input_path\n",
    "        self.input_file: str = input_file\n",
    "        self.output_path: str = output_path\n",
    "        self.use_gpu: bool = use_gpu and GPU_ENABLED\n",
    "        self.cuda_available: bool = CUDA_AVAILABLE\n",
    "        \n",
    "        # Log GPU status\n",
    "        if self.use_gpu and self.cuda_available:\n",
    "            print(\"CUDA GPU acceleration enabled\")\n",
    "        elif self.use_gpu and not self.cuda_available:\n",
    "            print(\"GPU acceleration requested but CUDA not available - using CPU\")\n",
    "            self.use_gpu = False\n",
    "        else:\n",
    "            print(\"Using CPU for computations\")\n",
    "        \n",
    "        # Data containers\n",
    "        self.data: Optional[pd.DataFrame] = None\n",
    "        self.geo_data: Optional[gpd.GeoDataFrame] = None  # Original data with geometry\n",
    "        self.numeric_data: Optional[pd.DataFrame] = None  # Processed data for clustering\n",
    "        self.id_data: Optional[pd.DataFrame] = None\n",
    "        \n",
    "        # Analysis results\n",
    "        self.cluster_results: Optional[pd.DataFrame] = None\n",
    "        self.pca_results: Optional[pd.DataFrame] = None\n",
    "        self.feature_importance: Optional[pd.DataFrame] = None\n",
    "        \n",
    "        # Model artifacts\n",
    "        self.best_model: Optional[XGBClassifier] = None\n",
    "        self.best_params: Optional[Dict[str, Any]] = None\n",
    "        self.cluster_labels: Optional[np.ndarray] = None\n",
    "        self.evaluation_metrics: Dict[str, Any] = {}\n",
    "        \n",
    "    def load_data(self) -> bool:\n",
    "        \"\"\"Load the geopackage data and perform initial data inspection.\n",
    "        \n",
    "        This method loads the geospatial data from the specified geopackage file,\n",
    "        performs basic data inspection, and saves column names for reference.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if data was loaded successfully, False otherwise\n",
    "        \"\"\"\n",
    "        file_path: str = os.path.join(self.input_path, self.input_file)\n",
    "        print(f\"Loading data from {file_path}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load the geospatial data\n",
    "            self.geo_data = gpd.read_file(file_path)\n",
    "            \n",
    "            if self.geo_data is None or self.geo_data.empty:\n",
    "                print(\"Error: Loaded GeoDataFrame is empty\")\n",
    "                return False\n",
    "                \n",
    "            print(f\"Successfully loaded data with shape: {self.geo_data.shape}\")\n",
    "            \n",
    "            # Create a copy without geometry for analysis\n",
    "            self.data = self.geo_data.copy()\n",
    "            \n",
    "            # Basic info about the dataset\n",
    "            print(\"\\n--- Dataset Overview ---\")\n",
    "            \n",
    "            tract_count: int = len(self.data)\n",
    "            attribute_count: int = len(self.data.columns)\n",
    "            \n",
    "            print(f\"Number of census tracts: {tract_count}\")\n",
    "            \n",
    "            print(f\"Number of attributes: {attribute_count}\")\n",
    "            \n",
    "            # Save the column names to a text file for reference\n",
    "            column_file_path: str = os.path.join(self.output_path, \"column_names.txt\")\n",
    "            \n",
    "            with open(column_file_path, \"w\") as f:\n",
    "                for col in self.data.columns:\n",
    "                    f.write(f\"{col}\\n\")\n",
    "            \n",
    "            print(f\"Column names saved to: {column_file_path}\")\n",
    "            return True\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found - {file_path}\")\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {str(e)}\")\n",
    "            \n",
    "            return False\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Prepare data for analysis with advanced data quality techniques:\n",
    "        1. Handling missing values with sophisticated imputation methods\n",
    "        2. Detecting and handling outliers\n",
    "        3. Ensuring values fall within reasonable bounds\n",
    "        4. Comprehensive data quality checks\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"No data loaded. Please load data first.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n--- Preprocessing Data ---\")\n",
    "        \n",
    "        # 1. Extract identifier columns before preprocessing\n",
    "        id_columns = ['GEOID', 'NAME', 'NAMELSAD', 'STATEFP', 'COUNTYFP', 'TRACTCE']\n",
    "        geo_column = 'geometry'\n",
    "        \n",
    "        # Save original identifiers for later use\n",
    "        self.id_data = self.data[id_columns].copy() if all(col in self.data.columns for col in id_columns) else None\n",
    "        \n",
    "        # 2. Determine which columns are numeric\n",
    "        numeric_columns = self.data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        # Remove id columns from numeric columns if they exist there\n",
    "        numeric_columns = [col for col in numeric_columns if col not in id_columns and col != geo_column]\n",
    "        \n",
    "        print(f\"Number of numeric columns for analysis: {len(numeric_columns)}\")\n",
    "        \n",
    "        # 3. Create a dataset with only numeric columns\n",
    "        numeric_data = self.data[numeric_columns].copy()\n",
    "        \n",
    "        # 4. Check for missing values - detailed analysis\n",
    "        missing_data = numeric_data.isnull().sum()\n",
    "        missing_cols = missing_data[missing_data > 0]\n",
    "        \n",
    "        if not missing_cols.empty:\n",
    "            print(f\"Found {len(missing_cols)} columns with missing values\")\n",
    "            print(\"Missing value statistics:\")\n",
    "            print(f\"  Min missing: {missing_cols.min()} ({missing_cols.min()/len(numeric_data)*100:.2f}%)\")\n",
    "            print(f\"  Max missing: {missing_cols.max()} ({missing_cols.max()/len(numeric_data)*100:.2f}%)\")\n",
    "            print(f\"  Average missing: {missing_cols.mean():.1f} ({missing_cols.mean()/len(numeric_data)*100:.2f}%)\")\n",
    "            \n",
    "            # Save missing data analysis to CSV\n",
    "            missing_data_analysis = pd.DataFrame({\n",
    "                'Column': missing_cols.index,\n",
    "                'Missing_Count': missing_cols.values,\n",
    "                'Missing_Percent': (missing_cols.values / len(numeric_data) * 100)\n",
    "            })\n",
    "            missing_data_analysis.to_csv(os.path.join(self.output_path, 'missing_data_analysis.csv'), index=False)\n",
    "            \n",
    "            # Drop columns with too many missing values (>80%)\n",
    "            high_missing_cols = missing_cols[missing_cols/len(numeric_data) > 0.8].index.tolist()\n",
    "            if high_missing_cols:\n",
    "                print(f\"Dropping {len(high_missing_cols)} columns with >80% missing values\")\n",
    "                numeric_data = numeric_data.drop(columns=high_missing_cols)\n",
    "        else:\n",
    "            print(\"No missing values found in the dataset\")\n",
    "        \n",
    "        # 5. Remove constant and near-constant columns (low variance)\n",
    "        # Calculate variance for each column\n",
    "        variances = numeric_data.var()\n",
    "        low_var_threshold = 0.01  # Threshold for what constitutes \"low variance\"\n",
    "        low_var_cols = variances[variances < low_var_threshold].index.tolist()\n",
    "        \n",
    "        if low_var_cols:\n",
    "            print(f\"Removing {len(low_var_cols)} columns with low variance\")\n",
    "            numeric_data = numeric_data.drop(columns=low_var_cols)\n",
    "        \n",
    "        # Save a copy of data before advanced processing for potential comparison\n",
    "        pre_cleaned_data = numeric_data.copy()\n",
    "        \n",
    "        # 6. Advanced missing value imputation\n",
    "        print(\"\\nPerforming advanced missing value imputation...\")\n",
    "        # Define different imputation strategies\n",
    "        \n",
    "        # Columns with < 10% missing: Use KNN imputation\n",
    "        # Columns with >= 10% missing: Use iterative imputation (MICE)\n",
    "        missing_pct = numeric_data.isnull().mean()\n",
    "        knn_cols = missing_pct[missing_pct < 0.1].index.tolist()\n",
    "        iter_cols = missing_pct[missing_pct >= 0.1].index.tolist()\n",
    "        \n",
    "        # Create copies for imputation\n",
    "        if knn_cols:\n",
    "            knn_data = numeric_data[knn_cols].copy()\n",
    "            # Apply KNN imputation\n",
    "            knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "            knn_imputed = pd.DataFrame(\n",
    "                knn_imputer.fit_transform(knn_data),\n",
    "                columns=knn_data.columns,\n",
    "                index=knn_data.index\n",
    "            )\n",
    "            numeric_data[knn_cols] = knn_imputed\n",
    "            print(f\"Applied KNN imputation to {len(knn_cols)} columns with <10% missing values\")\n",
    "        \n",
    "        if iter_cols:\n",
    "            iter_data = numeric_data[iter_cols].copy()\n",
    "            # Apply iterative imputation\n",
    "            iter_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "            iter_imputed = pd.DataFrame(\n",
    "                iter_imputer.fit_transform(iter_data),\n",
    "                columns=iter_data.columns,\n",
    "                index=iter_data.index\n",
    "            )\n",
    "            numeric_data[iter_cols] = iter_imputed\n",
    "            print(f\"Applied iterative imputation to {len(iter_cols)} columns with ≥10% missing values\")\n",
    "        \n",
    "        # 7. Check for and handle outliers using IQR\n",
    "        print(\"\\nDetecting and handling outliers...\")\n",
    "        \n",
    "        # Create a heatmap of potential outliers for visualization\n",
    "        outlier_matrix = pd.DataFrame(index=numeric_data.index)\n",
    "        \n",
    "        for col in numeric_data.columns:\n",
    "            # Calculate IQR\n",
    "            Q1 = numeric_data[col].quantile(0.25)\n",
    "            Q3 = numeric_data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define outlier bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Identify outliers\n",
    "            outliers = (numeric_data[col] < lower_bound) | (numeric_data[col] > upper_bound)\n",
    "            outlier_matrix[col] = outliers.astype(int)\n",
    "            \n",
    "            # Count outliers\n",
    "            outlier_count = outliers.sum()\n",
    "            outlier_pct = outlier_count / len(numeric_data) * 100\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                print(f\"  {col}: {outlier_count} outliers ({outlier_pct:.2f}%)\")\n",
    "                \n",
    "                # Handle outliers - cap at the bounds instead of removing\n",
    "                numeric_data.loc[numeric_data[col] < lower_bound, col] = lower_bound\n",
    "                numeric_data.loc[numeric_data[col] > upper_bound, col] = upper_bound\n",
    "        \n",
    "        # Calculate total outlier percentage by row\n",
    "        outlier_matrix['total_pct'] = outlier_matrix.sum(axis=1) / len(numeric_data.columns) * 100\n",
    "        \n",
    "        # Identify rows with excessive outliers (>50% of features are outliers)\n",
    "        excessive_outlier_rows = outlier_matrix[outlier_matrix['total_pct'] > 50].index.tolist()\n",
    "        \n",
    "        if excessive_outlier_rows:\n",
    "            print(f\"\\nIdentified {len(excessive_outlier_rows)} rows with >50% outlier features\")\n",
    "            print(\"  These rows might be anomalous - recording them for reference\")\n",
    "            \n",
    "            # Save these rows to a file for further inspection\n",
    "            if self.id_data is not None:\n",
    "                anomalous_data = pd.concat([self.id_data.loc[excessive_outlier_rows], \n",
    "                                           numeric_data.loc[excessive_outlier_rows]], axis=1)\n",
    "                anomalous_data.to_csv(os.path.join(self.output_path, 'anomalous_tracts.csv'), index=False)\n",
    "        \n",
    "        # 8. Use Isolation Forest as a secondary method to detect outliers\n",
    "        try:\n",
    "            print(\"\\nPerforming Isolation Forest outlier detection...\")\n",
    "            iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "            iso_forest.fit(numeric_data)\n",
    "            \n",
    "            # Get anomaly scores\n",
    "            outlier_scores = iso_forest.decision_function(numeric_data)\n",
    "            iso_outliers = iso_forest.predict(numeric_data) == -1\n",
    "            \n",
    "            print(f\"Isolation Forest identified {iso_outliers.sum()} potential outlier rows ({iso_outliers.mean()*100:.2f}%)\")\n",
    "            \n",
    "            geoid_series = self.id_data[\"GEOID\"].reset_index(drop=True)\n",
    "\n",
    "            # Save isolation forest outlier scores for reference\n",
    "            outlier_scores_df = pd.DataFrame({\n",
    "                'GEOID': geoid_series,\n",
    "                'Outlier_Score': outlier_scores,\n",
    "                'Is_Outlier': iso_outliers\n",
    "            })\n",
    "            outlier_scores_df.to_csv(os.path.join(self.output_path, 'isolation_forest_outliers.csv'), index=False)\n",
    "            \n",
    "            # For very extreme outliers detected by Isolation Forest, apply smoothing\n",
    "            extreme_outliers_idx = outlier_scores < -0.5  # More negative means more anomalous\n",
    "            if extreme_outliers_idx.sum() > 0:\n",
    "                print(f\"Applying robust scaling to {extreme_outliers_idx.sum()} extreme outlier rows\")\n",
    "                \n",
    "                # Apply RobustScaler to these extreme outliers to reduce their impact\n",
    "                robust_scaler = RobustScaler()\n",
    "                extreme_data = numeric_data.loc[extreme_outliers_idx].copy()\n",
    "                extreme_scaled = pd.DataFrame(\n",
    "                    robust_scaler.fit_transform(extreme_data),\n",
    "                    columns=extreme_data.columns,\n",
    "                    index=extreme_data.index\n",
    "                )\n",
    "                numeric_data.loc[extreme_outliers_idx] = extreme_scaled\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Isolation Forest analysis: {e}\")\n",
    "        \n",
    "        # 9. Confirm data consistency and range\n",
    "        print(\"\\nVerifying data consistency after preprocessing...\")\n",
    "        \n",
    "        # Check for any remaining nulls\n",
    "        remaining_nulls = numeric_data.isnull().sum().sum()\n",
    "        if remaining_nulls > 0:\n",
    "            print(f\"Warning: {remaining_nulls} null values remain after imputation!\")\n",
    "            print(\"Applying final median imputation to any remaining nulls\")\n",
    "            numeric_data = numeric_data.fillna(numeric_data.median())\n",
    "        else:\n",
    "            print(\"No null values remain - imputation successful\")\n",
    "        \n",
    "        # 10. Generate data quality report\n",
    "        quality_report = pd.DataFrame({\n",
    "            'Column': numeric_data.columns,\n",
    "            'Mean': numeric_data.mean(),\n",
    "            'Median': numeric_data.median(),\n",
    "            'Std': numeric_data.std(),\n",
    "            'Min': numeric_data.min(),\n",
    "            'Max': numeric_data.max(),\n",
    "            'Skewness': numeric_data.skew(),\n",
    "            'Kurtosis': numeric_data.kurtosis()\n",
    "        })\n",
    "        \n",
    "        # Save quality report\n",
    "        quality_report.to_csv(os.path.join(self.output_path, 'data_quality_report.csv'), index=False)\n",
    "        \n",
    "        # 11. Create correlation matrix for cleaned data\n",
    "        correlation = numeric_data.corr()\n",
    "        \n",
    "        # Identify extreme correlations (>0.95 or <-0.95)\n",
    "        extreme_corr_pairs = []\n",
    "        for i in range(len(correlation.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(correlation.iloc[i, j]) > 0.95:\n",
    "                    extreme_corr_pairs.append({\n",
    "                        'Feature1': correlation.columns[i],\n",
    "                        'Feature2': correlation.columns[j],\n",
    "                        'Correlation': correlation.iloc[i, j]\n",
    "                    })\n",
    "        \n",
    "        if extreme_corr_pairs:\n",
    "            print(f\"\\nIdentified {len(extreme_corr_pairs)} pairs of extremely correlated features (|r| > 0.95)\")\n",
    "            \n",
    "            # Save these for reference\n",
    "            extreme_corr_df = pd.DataFrame(extreme_corr_pairs)\n",
    "            extreme_corr_df.to_csv(os.path.join(self.output_path, 'extreme_correlations.csv'), index=False)\n",
    "            \n",
    "            # Consider dropping one from each highly correlated pair to reduce multicollinearity\n",
    "            # (doing this for top correlations only to avoid excessive feature removal)\n",
    "            if len(extreme_corr_pairs) > 5:\n",
    "                # Sort by absolute correlation\n",
    "                extreme_corr_df['abs_corr'] = extreme_corr_df['Correlation'].abs()\n",
    "                extreme_corr_df = extreme_corr_df.sort_values('abs_corr', ascending=False).drop('abs_corr', axis=1)\n",
    "                \n",
    "                # Take top 5 pairs\n",
    "                top_pairs = extreme_corr_df.head(5)\n",
    "                \n",
    "                # For each pair, keep the feature with higher variance\n",
    "                features_to_drop = []\n",
    "                for _, row in top_pairs.iterrows():\n",
    "                    feature1 = row['Feature1']\n",
    "                    feature2 = row['Feature2']\n",
    "                    \n",
    "                    if numeric_data[feature1].var() > numeric_data[feature2].var():\n",
    "                        features_to_drop.append(feature2)\n",
    "                    else:\n",
    "                        features_to_drop.append(feature1)\n",
    "                \n",
    "                # Remove duplicates\n",
    "                features_to_drop = list(set(features_to_drop))\n",
    "                \n",
    "                print(f\"Removing {len(features_to_drop)} features from highly correlated pairs\")\n",
    "                numeric_data = numeric_data.drop(columns=features_to_drop)\n",
    "        \n",
    "        # 12. Final step: Create visualization of data distribution after preprocessing\n",
    "        try:\n",
    "            # Visualize data distributions before and after preprocessing\n",
    "            fig = plt.figure(figsize=(15, 10))\n",
    "            gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n",
    "            \n",
    "            ax1 = plt.subplot(gs[0])\n",
    "            ax2 = plt.subplot(gs[1])\n",
    "            \n",
    "            # Sample a few columns for visualization (up to 5)\n",
    "            vis_columns = list(pre_cleaned_data.columns[:5])\n",
    "            \n",
    "            # Before preprocessing\n",
    "            pre_cleaned_data[vis_columns].boxplot(ax=ax1)\n",
    "            ax1.set_title('Data Distribution Before Preprocessing')\n",
    "            ax1.set_ylabel('Value')\n",
    "            ax1.set_xticklabels(vis_columns, rotation=45, ha='right')\n",
    "            \n",
    "            # After preprocessing\n",
    "            numeric_data[vis_columns].boxplot(ax=ax2)\n",
    "            ax2.set_title('Data Distribution After Preprocessing')\n",
    "            ax2.set_ylabel('Value')\n",
    "            ax2.set_xticklabels(vis_columns, rotation=45, ha='right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_path, 'preprocessing_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating preprocessing visualization: {e}\")\n",
    "        \n",
    "        # 13. Save final preprocessed dataset\n",
    "        numeric_data.to_csv(os.path.join(self.output_path, 'preprocessed_data.csv'), index=False)\n",
    "        \n",
    "        # Store the final preprocessed data\n",
    "        self.numeric_data = numeric_data\n",
    "        \n",
    "        print(f\"\\nFinal preprocessed data shape: {self.numeric_data.shape}\")\n",
    "        print(f\"Data is now pristine and ready for analysis\")\n",
    "        return True\n",
    "    \n",
    "    def perform_statistical_analysis(self):\n",
    "        \"\"\"Perform statistical analysis on the preprocessed data.\"\"\"\n",
    "        if self.numeric_data is None:\n",
    "            print(\"No preprocessed data available. Please preprocess data first.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n--- Statistical Analysis ---\")\n",
    "        \n",
    "        # 1. Basic descriptive statistics\n",
    "        desc_stats = self.numeric_data.describe().T\n",
    "        desc_stats['missing'] = self.numeric_data.isnull().sum()\n",
    "        desc_stats['missing_percent'] = (self.numeric_data.isnull().sum() / len(self.numeric_data)) * 100\n",
    "        \n",
    "        # Save descriptive statistics to CSV\n",
    "        desc_stats.to_csv(os.path.join(self.output_path, 'descriptive_statistics.csv'))\n",
    "        print(\"Saved descriptive statistics to CSV\")\n",
    "        \n",
    "        # 2. Calculate correlation matrix\n",
    "        correlation = self.numeric_data.corr()\n",
    "        \n",
    "        # Save correlation matrix to CSV\n",
    "        correlation.to_csv(os.path.join(self.output_path, 'correlation_matrix.csv'))\n",
    "        \n",
    "        # 3. Identify highly correlated features (absolute correlation > 0.8)\n",
    "        high_corr = pd.DataFrame(correlation.abs().unstack().sort_values(ascending=False))\n",
    "        high_corr = high_corr[high_corr[0] > 0.8]\n",
    "        high_corr = high_corr[high_corr.index.get_level_values(0) != high_corr.index.get_level_values(1)]\n",
    "        \n",
    "        # Save high correlations to CSV\n",
    "        high_corr.to_csv(os.path.join(self.output_path, 'high_correlations.csv'))\n",
    "        print(f\"Identified {len(high_corr)} highly correlated feature pairs\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def perform_feature_selection(self):\n",
    "        \"\"\"Select the most important features for clustering.\"\"\"\n",
    "        if self.numeric_data is None:\n",
    "            print(\"No preprocessed data available. Please preprocess data first.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n--- Feature Selection ---\")\n",
    "        \n",
    "        # 1. Normalize the data for feature selection\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(self.numeric_data)\n",
    "        \n",
    "        # 2. Use Random Forest to select important features\n",
    "        try:\n",
    "            # Using a Random Forest to identify important features\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            \n",
    "            # Create synthetic targets for classification (using KMeans with 5 clusters)\n",
    "            temp_kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "            synthetic_targets = temp_kmeans.fit_predict(scaled_data)\n",
    "            \n",
    "            # Fit the Random Forest\n",
    "            rf.fit(scaled_data, synthetic_targets)\n",
    "            \n",
    "            # Get feature importances\n",
    "            importances = rf.feature_importances_\n",
    "            feature_importances = pd.DataFrame({\n",
    "                'Feature': self.numeric_data.columns,\n",
    "                'Importance': importances\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            \n",
    "            # Save feature importances to CSV\n",
    "            feature_importances.to_csv(os.path.join(self.output_path, 'feature_importances.csv'), index=False)\n",
    "            print(f\"Identified and saved importances for {len(feature_importances)} features\")\n",
    "            \n",
    "            # Store feature importance for later use\n",
    "            self.feature_importance = feature_importances\n",
    "            \n",
    "            # Select top features that account for 80% of the importance\n",
    "            cumulative_importance = np.cumsum(feature_importances['Importance'])\n",
    "            threshold_idx = np.where(cumulative_importance >= 0.8)[0][0]\n",
    "            selected_features = feature_importances.iloc[:threshold_idx+1]['Feature'].tolist()\n",
    "            \n",
    "            print(f\"Selected {len(selected_features)} features that account for 80% of importance\")\n",
    "            \n",
    "            # Update numeric data to include only selected features\n",
    "            self.numeric_data = self.numeric_data[selected_features]\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature selection: {e}\")\n",
    "            # Fallback: Just use all features if selection fails\n",
    "            print(\"Using all available features\")\n",
    "            return True\n",
    "    \n",
    "    def perform_dimensionality_reduction(self):\n",
    "        \"\"\"Perform PCA for dimensionality reduction.\"\"\"\n",
    "        if self.numeric_data is None:\n",
    "            print(\"No preprocessed data available. Please preprocess data first.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n--- Dimensionality Reduction ---\")\n",
    "        \n",
    "        # 1. Normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(self.numeric_data)\n",
    "        \n",
    "        # 2. Perform selected dimensionality reduction methods for comparison\n",
    "        for method in DR_METHODS:\n",
    "            if method == \"pca\":\n",
    "                try:\n",
    "                    pca = PCA()\n",
    "                    pca_result = pca.fit_transform(scaled_data)\n",
    "                    explained_variance = pca.explained_variance_ratio_\n",
    "                    cumulative_variance = np.cumsum(explained_variance)\n",
    "                    n_components = np.where(cumulative_variance >= 0.8)[0][0] + 1\n",
    "                    print(f\"PCA: {n_components} components explain >80% of variance\")\n",
    "                    pca_df = pd.DataFrame(\n",
    "                        pca_result[:, :n_components],\n",
    "                        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "                    )\n",
    "                    components_df = pd.DataFrame(\n",
    "                        pca.components_.T[:, :n_components],\n",
    "                        columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "                        index=self.numeric_data.columns\n",
    "                    )\n",
    "                    components_df.to_csv(os.path.join(self.output_path, 'pca_components.csv'))\n",
    "                    self.pca_results = pca_df\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in PCA: {e}\")\n",
    "            elif method == \"umap\":\n",
    "                if umap is None:\n",
    "                    print(\"umap not installed; skipping UMAP\")\n",
    "                    continue\n",
    "                try:\n",
    "                    reducer = umap.UMAP(random_state=42)\n",
    "                    umap_result = reducer.fit_transform(scaled_data)\n",
    "                    df_umap = pd.DataFrame(\n",
    "                        umap_result,\n",
    "                        columns=[f'UMAP{i+1}' for i in range(umap_result.shape[1])]\n",
    "                    )\n",
    "                    df_umap.to_csv(os.path.join(self.output_path, 'umap_components.csv'), index=False)\n",
    "                    self.umap_results = df_umap\n",
    "                    print(f\"UMAP: reduced to {umap_result.shape[1]} dimensions\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in UMAP: {e}\")\n",
    "            else:\n",
    "                print(f\"Unknown DR method: {method}\")\n",
    "        return True\n",
    "    \n",
    "    def perform_clustering(self, max_clusters=7):\n",
    "        \"\"\"\n",
    "        Perform KMeans clustering to segment census tracts.\n",
    "        Uses the Elbow method and Silhouette score to determine optimal cluster count.\n",
    "        \n",
    "        Returns dict with optimal cluster information and scaled data for modeling.\n",
    "        \"\"\"\n",
    "        if self.numeric_data is None:\n",
    "            print(\"No preprocessed data available. Please preprocess data first.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n--- Clustering Analysis ---\")\n",
    "        \n",
    "        # 1. Normalize the data for clustering\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(self.numeric_data)\n",
    "        \n",
    "        # Store scaled data for later use in XGBoost\n",
    "        self.scaled_data = scaled_data\n",
    "        \n",
    "        # 2. Determine optimal number of clusters using Elbow method and Silhouette score\n",
    "        wcss = []  # Within-cluster sum of squares\n",
    "        silhouette_scores = []\n",
    "        cluster_models = {}  # Store models for each cluster count\n",
    "        cluster_assignments = {}  # Store cluster assignments for each count\n",
    "        \n",
    "        # Start from 2 clusters\n",
    "        cluster_range = range(2, max_clusters + 1)\n",
    "        \n",
    "        for n_clusters in cluster_range:\n",
    "            # Apply KMeans\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            kmeans.fit(scaled_data)\n",
    "            \n",
    "            # Store model and assignments\n",
    "            cluster_models[n_clusters] = kmeans\n",
    "            cluster_assignments[n_clusters] = kmeans.labels_\n",
    "            \n",
    "            # Calculate WCSS\n",
    "            wcss.append(kmeans.inertia_)\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            cluster_labels = kmeans.labels_\n",
    "            try:\n",
    "                silhouette_avg = silhouette_score(scaled_data, cluster_labels)\n",
    "                silhouette_scores.append(silhouette_avg)\n",
    "                print(f\"For n_clusters = {n_clusters}, the silhouette score is {silhouette_avg:.3f}\")\n",
    "            except:\n",
    "                silhouette_scores.append(0)\n",
    "        \n",
    "        # Find optimal number of clusters\n",
    "        # Method 1: Largest silhouette score\n",
    "        optimal_clusters_silhouette = cluster_range[silhouette_scores.index(max(silhouette_scores))]\n",
    "        \n",
    "        # Method 2: Elbow method (find point of diminishing returns)\n",
    "        # Approximate the \"elbow\" point using rate of change\n",
    "        wcss_diff = np.diff(wcss)\n",
    "        wcss_diff2 = np.diff(wcss_diff)  # Second derivative\n",
    "        elbow_idx = np.argmax(wcss_diff2) + 1 if len(wcss_diff2) > 0 else 0\n",
    "        optimal_clusters_elbow = cluster_range[elbow_idx] if elbow_idx < len(cluster_range) else cluster_range[0]\n",
    "        \n",
    "        # Use silhouette score method as primary, with elbow as fallback\n",
    "        self.optimal_clusters = optimal_clusters_silhouette\n",
    "        print(f\"Optimal number of clusters: {self.optimal_clusters} (silhouette method)\")\n",
    "        print(f\"Alternative by elbow method: {optimal_clusters_elbow}\")\n",
    "        \n",
    "        # Store all clustering information for later use\n",
    "        self.clustering_info = {\n",
    "            'optimal_clusters': self.optimal_clusters,\n",
    "            'alternative_clusters': optimal_clusters_elbow,\n",
    "            'silhouette_scores': silhouette_scores,\n",
    "            'wcss': wcss,\n",
    "            'cluster_models': cluster_models,\n",
    "            'cluster_assignments': cluster_assignments,\n",
    "            'cluster_range': list(cluster_range)\n",
    "        }\n",
    "        \n",
    "        # Define the specific cluster counts we want to use\n",
    "        desired_cluster_counts = CLUSTER_COUNTS  # Manually specified cluster counts\n",
    "        \n",
    "        # Make sure we have models for all our desired cluster counts\n",
    "        for n_clusters in desired_cluster_counts:\n",
    "            if n_clusters not in cluster_models:\n",
    "                print(f\"Creating KMeans model with {n_clusters} clusters...\")\n",
    "                kmeans_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                kmeans_model.fit(scaled_data)\n",
    "                cluster_models[n_clusters] = kmeans_model\n",
    "                cluster_assignments[n_clusters] = kmeans_model.labels_\n",
    "            \n",
    "        # Use 10 clusters for initial visualization and analysis\n",
    "        self._apply_clustering(DEFAULT_CLUSTER_COUNT, scaled_data)\n",
    "        \n",
    "        # Train XGBoost with multiple cluster counts as a grid search parameter\n",
    "        # Use our predefined cluster counts\n",
    "        cluster_range_for_grid = desired_cluster_counts  # Use the same cluster counts we created models for\n",
    "        \n",
    "        print(f\"Will evaluate {len(cluster_range_for_grid)} different cluster counts in XGBoost grid search: {cluster_range_for_grid}\")\n",
    "        \n",
    "        # Train the model with multiple cluster counts as a hyperparameter\n",
    "        self.train_xgboost_model(scaled_data, cluster_assignments, cluster_range_for_grid)\n",
    "        \n",
    "        print(f\"Saved clustering results to CSV and GeoJSON\")\n",
    "        return True\n",
    "    \n",
    "    def _apply_clustering(self, n_clusters, scaled_data):\n",
    "        \"\"\"\n",
    "        Helper method to apply a specific clustering result to the data.\n",
    "        Uses pre-computed cluster assignments.\n",
    "        \"\"\"\n",
    "        # Get the labels for this cluster count\n",
    "        cluster_labels = self.clustering_info['cluster_assignments'][n_clusters]\n",
    "        \n",
    "        # Save the cluster labels for modeling\n",
    "        self.cluster_labels = cluster_labels\n",
    "        \n",
    "        # Add cluster labels to the original data\n",
    "        self.cluster_results = self.data.copy()\n",
    "        self.cluster_results['cluster'] = cluster_labels\n",
    "        \n",
    "        # Also add to the geo_data\n",
    "        self.geo_data['cluster'] = cluster_labels\n",
    "        \n",
    "        # Analyze clusters\n",
    "        cluster_analysis = pd.DataFrame()\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_data = self.numeric_data[self.cluster_results['cluster'] == cluster]\n",
    "            cluster_analysis[f'Cluster {cluster}'] = cluster_data.mean()\n",
    "        \n",
    "        # Save cluster analysis\n",
    "        cluster_analysis.to_csv(os.path.join(self.output_path, f'cluster_profiles_{n_clusters}.csv'))\n",
    "        print(f\"Saved cluster profiles for {n_clusters} clusters to CSV\")\n",
    "        \n",
    "        # Save results to CSV and GeoJSON for mapping\n",
    "        self.geo_data.to_file(os.path.join(self.output_path, f'clustered_tracts_{n_clusters}.geojson'), driver='GeoJSON')\n",
    "        \n",
    "        # Extract just the cluster and identifier columns for a lightweight CSV\n",
    "        result_df = self.cluster_results[['GEOID', 'STATEFP', 'COUNTYFP', 'TRACTCE', 'cluster']].copy()\n",
    "        result_df.to_csv(os.path.join(self.output_path, f'tract_clusters_{n_clusters}.csv'), index=False)\n",
    "    \n",
    "    def find_doppelgangers(self, n_doppelgangers=10):\n",
    "        \"\"\"\n",
    "        Find census tracts that are similar based on non-geographic characteristics \n",
    "        but located in different geographic areas.\n",
    "        \"\"\"\n",
    "        if self.cluster_results is None:\n",
    "            print(\"No clustering results available. Please perform clustering first.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n--- Finding Doppelganger Census Tracts ---\")\n",
    "        \n",
    "        # 1. Group by cluster and state\n",
    "        cluster_state_counts = self.cluster_results.groupby(['cluster', 'STATEFP']).size().reset_index(name='count')\n",
    "        \n",
    "        # 2. Find clusters that span multiple states\n",
    "        multi_state_clusters = cluster_state_counts.groupby('cluster').size()\n",
    "        multi_state_clusters = multi_state_clusters[multi_state_clusters > 1].index.tolist()\n",
    "        \n",
    "        print(f\"Found {len(multi_state_clusters)} clusters that span multiple states\")\n",
    "        \n",
    "        # Use tqdm for progress tracking if available\n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            multi_state_clusters_iter = tqdm(multi_state_clusters, desc=\"Processing clusters for doppelgangers\")\n",
    "        else:\n",
    "            multi_state_clusters_iter = multi_state_clusters\n",
    "        \n",
    "        if not multi_state_clusters:\n",
    "            print(\"No doppelgangers found (no clusters span multiple states)\")\n",
    "            return False\n",
    "        \n",
    "        # 3. For each multi-state cluster, find representative tracts\n",
    "        doppelgangers = []\n",
    "        \n",
    "        for cluster in multi_state_clusters_iter:\n",
    "            # Get tracts in this cluster\n",
    "            cluster_tracts = self.cluster_results[self.cluster_results['cluster'] == cluster]\n",
    "            \n",
    "            # Group by state\n",
    "            states = cluster_tracts['STATEFP'].unique()\n",
    "            \n",
    "            # If this cluster has tracts in multiple states\n",
    "            if len(states) >= 2:\n",
    "                # For each state pair, find doppelgangers\n",
    "                for i in range(len(states)):\n",
    "                    for j in range(i+1, len(states)):\n",
    "                        state1 = states[i]\n",
    "                        state2 = states[j]\n",
    "                        \n",
    "                        # Get tracts from each state\n",
    "                        state1_tracts = cluster_tracts[cluster_tracts['STATEFP'] == state1]\n",
    "                        state2_tracts = cluster_tracts[cluster_tracts['STATEFP'] == state2]\n",
    "                        \n",
    "                        # Sample a few tracts from each state (or take all if less than n_doppelgangers)\n",
    "                        sample_size = min(n_doppelgangers, len(state1_tracts), len(state2_tracts))\n",
    "                        \n",
    "                        if sample_size > 0:\n",
    "                            state1_sample = state1_tracts.sample(sample_size, random_state=42)\n",
    "                            state2_sample = state2_tracts.sample(sample_size, random_state=42)\n",
    "                            \n",
    "                            # Combine samples\n",
    "                            for idx in range(sample_size):\n",
    "                                doppelgangers.append({\n",
    "                                    'cluster': cluster,\n",
    "                                    'tract1_geoid': state1_sample.iloc[idx]['GEOID'],\n",
    "                                    'tract1_name': state1_sample.iloc[idx]['NAME'],\n",
    "                                    'tract1_state': state1,\n",
    "                                    'tract2_geoid': state2_sample.iloc[idx]['GEOID'],\n",
    "                                    'tract2_name': state2_sample.iloc[idx]['NAME'],\n",
    "                                    'tract2_state': state2\n",
    "                                })\n",
    "        \n",
    "        # Create dataframe of doppelgangers\n",
    "        doppelgangers_df = pd.DataFrame(doppelgangers)\n",
    "        \n",
    "        # Save doppelgangers to CSV\n",
    "        if len(doppelgangers) > 0:\n",
    "            doppelgangers_df.to_csv(os.path.join(self.output_path, 'doppelganger_tracts.csv'), index=False)\n",
    "            print(f\"Found and saved {len(doppelgangers)} doppelganger pairs\")\n",
    "            # Sampling-based done\n",
    "        else:\n",
    "            print(\"No sampling-based doppelgangers found\")\n",
    "        \n",
    "        # NearestNeighbors-based doppelganger search\n",
    "        if \"nearest_neighbors\" in DOPPELGANGER_METHODS:\n",
    "            print(\"\\n--- Finding Doppelganger via NearestNeighbors ---\")\n",
    "            nbrs = NearestNeighbors(n_neighbors=n_doppelgangers+1, metric=SIMILARITY_METRIC)\n",
    "            nbrs.fit(self.numeric_data.values)\n",
    "            distances, indices = nbrs.kneighbors(self.numeric_data.values)\n",
    "            similarities = 1 - distances\n",
    "            nn_pairs = []\n",
    "            for i, (dist_row, idx_row) in enumerate(zip(distances, indices)):\n",
    "                for dist, j in zip(dist_row[1:], idx_row[1:]):\n",
    "                    sim = 1 - dist\n",
    "                    if sim >= DOPPELGANGER_SIMILARITY_THRESHOLD and self.cluster_results.loc[i, 'STATEFP'] != self.cluster_results.loc[j, 'STATEFP']:\n",
    "                        nn_pairs.append({\n",
    "                            'cluster': self.cluster_results.loc[i, 'cluster'],\n",
    "                            'tract1_geoid': self.cluster_results.loc[i, 'GEOID'],\n",
    "                            'tract1_state': self.cluster_results.loc[i, 'STATEFP'],\n",
    "                            'tract2_geoid': self.cluster_results.loc[j, 'GEOID'],\n",
    "                            'tract2_state': self.cluster_results.loc[j, 'STATEFP'],\n",
    "                            'similarity': float(sim),\n",
    "                        })\n",
    "            nn_df = pd.DataFrame(nn_pairs).sort_values('similarity', ascending=False)\n",
    "            nn_df.to_csv(os.path.join(self.output_path, 'doppelganger_nn.csv'), index=False)\n",
    "            print(f\"Saved {len(nn_df)} NearestNeighbors-based pairs\")\n",
    "        return True\n",
    "    \n",
    "    def find_precise_doppelgangers(self) -> bool:\n",
    "        \"\"\"Find doppelgangers using direct, weighted similarity for high-accuracy matching.\"\"\"\n",
    "        if self.numeric_data is None or self.cluster_results is None:\n",
    "            print(\"No data or clustering results. Please run preprocessing and clustering first.\")\n",
    "            return False\n",
    "        print(\"\\n--- Finding Precise Doppelganger Tracts ---\")\n",
    "        # Scale data\n",
    "        scaler = StandardScaler()\n",
    "        scaled = scaler.fit_transform(self.numeric_data)\n",
    "        # Apply feature weights\n",
    "        weights = np.array([FEATURE_WEIGHTS.get(col, FEATURE_WEIGHT_DEFAULT)\n",
    "                            for col in self.numeric_data.columns])\n",
    "        weighted = scaled * weights\n",
    "        weighted_df = pd.DataFrame(weighted, index=self.numeric_data.index)\n",
    "        # Identify multi-state clusters\n",
    "        cluster_state_counts = self.cluster_results.groupby(['cluster', 'STATEFP']).size().reset_index(name='count')\n",
    "        multi_state = cluster_state_counts.groupby('cluster').size()\n",
    "        multi_state = multi_state[multi_state > 1].index.tolist()\n",
    "        all_pairs = []\n",
    "        for cluster in multi_state:\n",
    "            subset = self.cluster_results[self.cluster_results['cluster'] == cluster]\n",
    "            states = subset['STATEFP'].unique()\n",
    "            for i in range(len(states)):\n",
    "                for j in range(i+1, len(states)):\n",
    "                    s1, s2 = states[i], states[j]\n",
    "                    idx1 = subset[subset['STATEFP']==s1].index\n",
    "                    idx2 = subset[subset['STATEFP']==s2].index\n",
    "                    feat1 = weighted_df.loc[idx1].values\n",
    "                    feat2 = weighted_df.loc[idx2].values\n",
    "                    sim = cosine_similarity(feat1, feat2)\n",
    "                    for m, tract1 in enumerate(idx1):\n",
    "                        top_idx = np.argsort(-sim[m])[:DOPPELGANGER_SAMPLE_SIZE]\n",
    "                        for n in top_idx:\n",
    "                            score = sim[m, n]\n",
    "                            if score >= DOPPELGANGER_SIMILARITY_THRESHOLD:\n",
    "                                all_pairs.append({\n",
    "                                    'cluster': cluster,\n",
    "                                    'tract1_geoid': tract1,\n",
    "                                    'tract1_state': s1,\n",
    "                                    'tract2_geoid': idx2[n],\n",
    "                                    'tract2_state': s2,\n",
    "                                    'similarity_score': score\n",
    "                                })\n",
    "        df = pd.DataFrame(all_pairs)\n",
    "        if not df.empty:\n",
    "            df.to_csv(os.path.join(self.output_path, 'precise_doppelganger_tracts.csv'), index=False)\n",
    "            # Evaluation metrics\n",
    "            mean_sim = df['similarity_score'].mean()\n",
    "            min_sim = df['similarity_score'].min()\n",
    "            max_sim = df['similarity_score'].max()\n",
    "            summary = pd.DataFrame([{\n",
    "                'mean_similarity': mean_sim,\n",
    "                'min_similarity': min_sim,\n",
    "                'max_similarity': max_sim,\n",
    "                'num_pairs': len(df)\n",
    "            }])\n",
    "            summary.to_csv(os.path.join(self.output_path, 'doppelganger_similarity_summary.csv'), index=False)\n",
    "            print(f\"Found {len(df)} precise doppelganger pairs (mean sim: {mean_sim:.3f})\")\n",
    "            return True\n",
    "        print(\"No precise doppelgangers found\")\n",
    "        return False\n",
    "    \n",
    "    def train_xgboost_model(self, X, y_dict, cluster_range):\n",
    "        \"\"\"\n",
    "        Train an XGBoost model to predict clusters using grid search for hyperparameter optimization.\n",
    "        Tests multiple optimizers including Adam and utilizes GPU acceleration if available.\n",
    "        Tests different cluster counts separately to find the optimal configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Feature matrix (scaled data)\n",
    "        y_dict : dict\n",
    "            Dictionary of cluster labels from KMeans, keyed by number of clusters\n",
    "        cluster_range : list\n",
    "            List of cluster counts to test in the grid search\n",
    "        \"\"\"\n",
    "        print(\"\\n--- XGBoost Classification with Grid Search ---\")\n",
    "        \n",
    "        # We'll use the first cluster count for initial data splitting\n",
    "        # The actual model will be trained with different cluster counts\n",
    "        first_cluster_count = cluster_range[0]\n",
    "        primary_labels = y_dict[first_cluster_count]\n",
    "        \n",
    "        # Split data using the primary clustering as stratification guide\n",
    "        X_train, X_test, y_train_idx, y_test_idx = train_test_split(\n",
    "            X, np.arange(len(primary_labels)), test_size=0.3, random_state=42, stratify=primary_labels\n",
    "        )\n",
    "        \n",
    "        # Create training and test dictionaries for each cluster count\n",
    "        self.y_train_dict = {}\n",
    "        self.y_test_dict = {}\n",
    "        \n",
    "        for n_clusters in cluster_range:\n",
    "            cluster_labels = y_dict[n_clusters]\n",
    "            self.y_train_dict[n_clusters] = cluster_labels[y_train_idx]\n",
    "            self.y_test_dict[n_clusters] = cluster_labels[y_test_idx]\n",
    "        \n",
    "        print(f\"Training XGBoost classifier with {len(cluster_range)} different cluster counts: {cluster_range}\")\n",
    "        \n",
    "        # Define parameter grid for grid search (excluding cluster counts)\n",
    "        # Testing different optimizers and hyperparameters\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.01, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'n_estimators': [100, 200],\n",
    "            'reg_alpha': [0, 1],\n",
    "            'reg_lambda': [0, 10],\n",
    "            'subsample': [0.6, 0.8],\n",
    "            'colsample_bytree': [0.6, 0.8],\n",
    "        }\n",
    "        \n",
    "        # Add GPU-specific parameters if CUDA is available\n",
    "        if self.use_gpu and self.cuda_available:\n",
    "            # For GPU, we'll use fixed parameters rather than grid search options to maximize GPU efficiency\n",
    "            param_grid['tree_method'] = ['gpu_hist']\n",
    "            print(\"Using CUDA GPU acceleration for XGBoost\")\n",
    "        else:\n",
    "            # CPU-only methods - more options for grid search\n",
    "            param_grid['tree_method'] = ['auto', 'exact', 'approx', 'hist']\n",
    "            print(\"Using CPU for XGBoost computation\")\n",
    "        \n",
    "        # Create separate models for each cluster count\n",
    "        models = {}\n",
    "        scores = {}\n",
    "        best_params_by_cluster = {}\n",
    "        \n",
    "        # Configure number of jobs based on whether GPU is used\n",
    "        # With GPU, we don't want to use multiple workers as they'll compete for GPU resources\n",
    "        n_jobs = 1 if (self.use_gpu and self.cuda_available) else -1\n",
    "        \n",
    "        # Set up cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Start timing the grid search\n",
    "        start_time = time()\n",
    "        \n",
    "        # Calculate total parameter combinations for progress tracking\n",
    "        param_combinations = 1\n",
    "        for param_values in param_grid.values():\n",
    "            param_combinations *= len(param_values)\n",
    "        \n",
    "        print(f\"Grid search will evaluate {param_combinations} parameter combinations for each of {len(cluster_range)} cluster counts\")\n",
    "        print(f\"Using {cv.get_n_splits()} cross-validation folds\")\n",
    "        total_fits = param_combinations * cv.get_n_splits() * len(cluster_range)\n",
    "        print(f\"Total of {total_fits} model fits will be performed\")\n",
    "        \n",
    "        # Define XGBoost callback for training progress if tqdm is available\n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            class TqdmProgressCallback(xgb.callback.TrainingCallback):\n",
    "                def __init__(self, max_rounds=100, show_interval=10):\n",
    "                    self.pbar = None\n",
    "                    self.max_rounds = max_rounds\n",
    "                    self.show_interval = show_interval\n",
    "                    \n",
    "                def before_training(self, model):\n",
    "                    self.pbar = tqdm(total=self.max_rounds, desc=\"XGBoost Training\", leave=False)\n",
    "                    return model\n",
    "                    \n",
    "                def after_iteration(self, model, epoch, evals_log):\n",
    "                    self.pbar.update(1)\n",
    "                    return False  # Continue training\n",
    "                    \n",
    "                def after_training(self, model):\n",
    "                    self.pbar.close()\n",
    "                    return model\n",
    "        \n",
    "        # Setup progress bars for outer loop (cluster counts)\n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            outer_pbar = tqdm(total=len(cluster_range), desc=\"Cluster Count Progress\")\n",
    "            inner_pbar = tqdm(total=param_combinations * cv.get_n_splits(), desc=\"Grid Search Progress\", leave=False)\n",
    "        \n",
    "        # Process each cluster count separately\n",
    "        for n_clusters in cluster_range:\n",
    "            print(f\"\\nTraining models for {n_clusters} clusters\")\n",
    "            \n",
    "            # Create XGBoost classifier with correct number of classes\n",
    "            current_params = {\n",
    "                'random_state': 42, \n",
    "                'eval_metric': 'mlogloss',\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': n_clusters\n",
    "            }\n",
    "            \n",
    "            # Add GPU parameters if available\n",
    "            if self.use_gpu and self.cuda_available:\n",
    "                current_params['tree_method'] = 'gpu_hist'\n",
    "                current_params['gpu_id'] = 0\n",
    "                current_params['predictor'] = 'gpu_predictor'\n",
    "            \n",
    "            xgb_model = XGBClassifier(**current_params)\n",
    "            \n",
    "            # Add callbacks for this model if using GPU and progress tracking\n",
    "            if TQDM_AVAILABLE and USE_PROGRESS_BARS and self.use_gpu and self.cuda_available:\n",
    "                xgb_model.set_params(callbacks=[TqdmProgressCallback(200)])  # Assume max of 200 rounds\n",
    "            \n",
    "            # Create grid search for this cluster count\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=xgb_model,\n",
    "                param_grid=param_grid,\n",
    "                cv=cv,\n",
    "                scoring='f1_weighted',\n",
    "                n_jobs=n_jobs,  # Number of parallel jobs\n",
    "                verbose=1 if not (TQDM_AVAILABLE and USE_PROGRESS_BARS) else 0  # Only use verbose if tqdm not available\n",
    "            )\n",
    "            \n",
    "            # Fit grid search for this cluster count\n",
    "            try:\n",
    "                if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "                    # Reset inner progress bar\n",
    "                    inner_pbar.reset()\n",
    "                    inner_pbar.set_description(f\"Grid Search for {n_clusters} clusters\")\n",
    "                    \n",
    "                    # Monkey patch the fit method to track progress\n",
    "                    original_fit = xgb_model.fit\n",
    "                    \n",
    "                    def fit_with_progress(*args, **kwargs):\n",
    "                        result = original_fit(*args, **kwargs)\n",
    "                        inner_pbar.update(1)\n",
    "                        return result\n",
    "                    \n",
    "                    # Apply the monkey patch\n",
    "                    try:\n",
    "                        xgb_model.fit = fit_with_progress\n",
    "                        grid_search.fit(X_train, self.y_train_dict[n_clusters])\n",
    "                    finally:\n",
    "                        # Restore original method\n",
    "                        xgb_model.fit = original_fit\n",
    "                else:\n",
    "                    grid_search.fit(X_train, self.y_train_dict[n_clusters])\n",
    "                \n",
    "                # Store best model and score for this cluster count\n",
    "                models[n_clusters] = grid_search.best_estimator_\n",
    "                scores[n_clusters] = grid_search.best_score_\n",
    "                best_params_by_cluster[n_clusters] = grid_search.best_params_\n",
    "                \n",
    "                print(f\"Best score for {n_clusters} clusters: {grid_search.best_score_:.4f}\")\n",
    "                print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training model for {n_clusters} clusters: {e}\")\n",
    "                continue\n",
    "                \n",
    "            if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "                outer_pbar.update(1)\n",
    "        \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            outer_pbar.close()\n",
    "            inner_pbar.close()\n",
    "        \n",
    "        # Report grid search time and results\n",
    "        grid_search_time = time() - start_time\n",
    "        print(f\"\\nGrid search completed in {grid_search_time:.2f} seconds\")\n",
    "        \n",
    "        # Find the best cluster count based on cross-validation scores\n",
    "        if scores:\n",
    "            best_cluster_count = max(scores, key=scores.get)\n",
    "            best_score = scores[best_cluster_count]\n",
    "            print(f\"Best overall cluster count: {best_cluster_count} with score: {best_score:.4f}\")\n",
    "            \n",
    "            # Save the best model and parameters\n",
    "            self.best_model = models[best_cluster_count]\n",
    "            self.best_params = best_params_by_cluster[best_cluster_count]\n",
    "            self.best_params['num_clusters'] = best_cluster_count\n",
    "            \n",
    "            print(\"\\nBest parameters found:\")\n",
    "            for param, value in self.best_params.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "            \n",
    "            # Now evaluate with the test labels for the best cluster count\n",
    "            y_test = self.y_test_dict[best_cluster_count]\n",
    "            y_pred = self.best_model.predict(X_test)\n",
    "        else:\n",
    "            print(\"No successful models were trained. Using default cluster count.\")\n",
    "            best_cluster_count = self.optimal_clusters\n",
    "            y_test = self.y_test_dict[best_cluster_count]\n",
    "            y_pred = None\n",
    "            \n",
    "        # Calculate metrics if we have predictions\n",
    "        if y_pred is not None:\n",
    "            try:\n",
    "                # Ensure both are integer arrays with same format\n",
    "                y_test_int = y_test.astype(int) if hasattr(y_test, 'astype') else np.array(y_test, dtype=int)\n",
    "                y_pred_int = y_pred.astype(int) if hasattr(y_pred, 'astype') else np.array(y_pred, dtype=int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test_int, y_pred_int)\n",
    "                f1 = f1_score(y_test_int, y_pred_int, average='weighted')\n",
    "                \n",
    "                print(f\"\\nModel Evaluation Metrics:\")\n",
    "                print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"  F1 Score (weighted): {f1:.4f}\")\n",
    "                print(f\"  Cluster Count: {best_cluster_count}\")\n",
    "                \n",
    "                # Store metrics\n",
    "                self.evaluation_metrics = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'f1_score': f1,\n",
    "                    'best_params': self.best_params\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating metrics: {e}\")\n",
    "                self.evaluation_metrics = {\n",
    "                    'error': f\"Error calculating metrics: {e}\",\n",
    "                    'best_params': self.best_params\n",
    "                }\n",
    "        else:\n",
    "            print(f\"\\nNo model evaluation metrics available\")\n",
    "            self.evaluation_metrics = {\n",
    "                'error': 'No successful model training',\n",
    "                'best_cluster_count': best_cluster_count\n",
    "            }\n",
    "            \n",
    "        # Apply the best cluster count to update our data\n",
    "        # This ensures all results are based on the optimal cluster count\n",
    "        if y_pred is not None:\n",
    "            # Apply the best clustering to the data\n",
    "            self._apply_clustering(best_cluster_count, X)\n",
    "        \n",
    "            # Compare all cluster counts\n",
    "            print(\"\\nScores by cluster count:\")\n",
    "            for n_clusters in sorted(scores.keys()):\n",
    "                print(f\"  {n_clusters} clusters: {scores[n_clusters]:.4f}\")\n",
    "        \n",
    "            # Save detailed classification report\n",
    "            try:\n",
    "                # Convert labels to ensure they have consistent format\n",
    "                y_test_int = y_test.astype(int) if hasattr(y_test, 'astype') else np.array(y_test, dtype=int)\n",
    "                y_pred_int = y_pred.astype(int) if hasattr(y_pred, 'astype') else np.array(y_pred, dtype=int)\n",
    "                \n",
    "                class_report = classification_report(y_test_int, y_pred_int, output_dict=True)\n",
    "                report_df = pd.DataFrame(class_report).transpose()\n",
    "                report_df.to_csv(os.path.join(self.output_path, f\"classification_report_{best_cluster_count}_clusters.csv\"))\n",
    "                print(\"Classification report saved successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating classification report: {e}\")\n",
    "            # Save confusion matrix\n",
    "            try:\n",
    "                y_test_int = y_test.astype(int) if hasattr(y_test, 'astype') else np.array(y_test, dtype=int)\n",
    "                y_pred_int = y_pred.astype(int) if hasattr(y_pred, 'astype') else np.array(y_pred, dtype=int)\n",
    "                \n",
    "                conf_matrix = confusion_matrix(y_test_int, y_pred_int)\n",
    "                np.savetxt(os.path.join(self.output_path, f\"confusion_matrix_{best_cluster_count}_clusters.csv\"), \n",
    "                          conf_matrix, delimiter=',', fmt='%d')\n",
    "                print(\"Confusion matrix saved successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating confusion matrix: {e}\")\n",
    "            \n",
    "            # Save feature importance if using a tree-based model\n",
    "            try:\n",
    "                feature_importance = self.best_model.feature_importances_\n",
    "                # Convert to DataFrame for better visualization\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'Feature': range(X.shape[1]),  # Feature indices as we don't have names\n",
    "                    'Importance': feature_importance\n",
    "                })\n",
    "                importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "                importance_df.to_csv(os.path.join(self.output_path, \"feature_importance.csv\"), index=False)\n",
    "                \n",
    "                # Plot feature importance\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "                plt.title(f'Top 20 Feature Importance - {best_cluster_count} clusters')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(self.output_path, \"feature_importance.png\"), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not save feature importance: {e}\")\n",
    "                \n",
    "            # Save model\n",
    "            try:\n",
    "                model_json_path = os.path.join(self.output_path, 'xgboost_model.json')\n",
    "                model_binary_path = os.path.join(self.output_path, 'xgboost_model.bin')\n",
    "                \n",
    "                self.best_model.save_model(model_json_path)\n",
    "                print(f\"XGBoost model saved to: {model_json_path}\")\n",
    "                \n",
    "                # Also save in binary format\n",
    "                self.best_model.save_model(model_binary_path)\n",
    "                print(f\"XGBoost model also saved to: {model_binary_path}\")\n",
    "                \n",
    "                # Save model configuration separately\n",
    "                model_config = {\n",
    "                    'best_params': self.best_params,\n",
    "                    'gpu_enabled': self.use_gpu,\n",
    "                    'cuda_available': self.cuda_available,\n",
    "                    'num_features': X.shape[1],\n",
    "                    'optimal_clusters': self.best_params['num_clusters'],\n",
    "                    'tested_cluster_counts': cluster_range\n",
    "                }\n",
    "                \n",
    "                pd.DataFrame([model_config]).to_csv(\n",
    "                    os.path.join(self.output_path, 'model_config.csv'), index=False\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model: {e}\")\n",
    "        \n",
    "        print(\"XGBoost training and evaluation complete\")\n",
    "        return True\n",
    "        \n",
    "    def analyze_feature_clusters(self):\n",
    "        \"\"\"\n",
    "        Analyze how features contribute to different clusters using the trained XGBoost model.\n",
    "        \"\"\"\n",
    "        if self.best_model is None or self.numeric_data is None:\n",
    "            print(\"No model or data available. Please train model first.\")\n",
    "            return False\n",
    "            \n",
    "        print(\"\\n--- Feature-Cluster Analysis ---\")\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = self.numeric_data.columns.tolist()\n",
    "        \n",
    "        # Create feature importance for each cluster using SHAP if available\n",
    "        try:\n",
    "            import shap\n",
    "            print(\"Using SHAP for feature importance analysis...\")\n",
    "            \n",
    "            # Create explainer\n",
    "            explainer = shap.TreeExplainer(self.best_model)\n",
    "            \n",
    "            # Need to ensure we're using the same features the model was trained on\n",
    "            print(f\"Model trained with {self.best_model.n_features_in_} features\")\n",
    "            print(f\"Current feature set has {len(feature_names)} features\")\n",
    "            \n",
    "            # Only select the subset of features the model was trained on\n",
    "            if hasattr(self.best_model, 'n_features_in_'):\n",
    "                if len(feature_names) > self.best_model.n_features_in_:\n",
    "                    feature_names = feature_names[:self.best_model.n_features_in_]\n",
    "                    print(f\"Using first {len(feature_names)} features for SHAP analysis\")\n",
    "            \n",
    "            # Sample data for SHAP analysis (use a subset if dataset is large)\n",
    "            sample_data = self.numeric_data[feature_names].sample(\n",
    "                min(1000, len(self.numeric_data)), random_state=42\n",
    "            )\n",
    "            X_sample = StandardScaler().fit_transform(sample_data)\n",
    "            \n",
    "            try:\n",
    "                # Calculate SHAP values\n",
    "                shap_values = explainer.shap_values(X_sample)\n",
    "                \n",
    "                # Create and save SHAP summary plot\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(self.output_path, 'shap_summary.png'), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "                # Create cluster-specific SHAP analysis\n",
    "                for cluster in range(len(shap_values)):\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    shap.summary_plot(\n",
    "                        shap_values[cluster], X_sample, feature_names=feature_names, \n",
    "                        show=False, plot_type='bar'\n",
    "                    )\n",
    "                    plt.title(f\"Feature Importance for Cluster {cluster}\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(\n",
    "                        os.path.join(self.output_path, f'shap_cluster_{cluster}.png'), \n",
    "                        dpi=300, bbox_inches='tight'\n",
    "                    )\n",
    "                    plt.close()\n",
    "                    \n",
    "                print(\"SHAP analysis complete. Plots saved to output directory.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during SHAP visualization: {e}\")\n",
    "                print(\"Falling back to standard feature importance analysis\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"SHAP package not available. Using standard feature importance analysis.\")\n",
    "            \n",
    "            # Use built-in feature importance\n",
    "            feature_importance = self.best_model.feature_importances_\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': feature_importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "            plt.title(\"Top 20 Feature Importances\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_path, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def run_full_analysis(self) -> bool:\n",
    "        \"\"\"Run the complete analysis pipeline.\n",
    "        \n",
    "        This method executes the full tract segmentation workflow, including:\n",
    "        - Data loading and preprocessing\n",
    "        - Statistical analysis\n",
    "        - Feature selection\n",
    "        - Dimensionality reduction\n",
    "        - Clustering\n",
    "        - XGBoost model training with hyperparameter optimization\n",
    "        - Feature-cluster relationship analysis\n",
    "        - Doppelganger identification\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if the full analysis completed successfully, False otherwise\n",
    "        \"\"\"\n",
    "        print(\"Starting full tract segmentation analysis...\")\n",
    "        \n",
    "        # Track timing for performance analysis\n",
    "        start_time: float = time()\n",
    "        \n",
    "        # Create progress tracking for overall workflow if tqdm is available\n",
    "        pipeline_steps = [\n",
    "            \"Data Loading\", \n",
    "            \"Preprocessing\", \n",
    "            \"Statistical Analysis\", \n",
    "            \"Feature Selection\",\n",
    "            \"Dimensionality Reduction\", \n",
    "            \"Clustering\", \n",
    "            \"Feature-Cluster Analysis\", \n",
    "            \"Doppelganger Identification\"\n",
    "        ]\n",
    "        \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress = tqdm(total=len(pipeline_steps), desc=\"Analysis Pipeline Progress\")\n",
    "        \n",
    "        # 1. Load the data\n",
    "        if not self.load_data():\n",
    "            print(\"ERROR: Data loading failed. Analysis aborted.\")\n",
    "            return False\n",
    "        \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress.update(1)\n",
    "            workflow_progress.set_description(f\"Completed: {pipeline_steps[0]}\")\n",
    "        \n",
    "        # 2. Preprocess the data\n",
    "        if not self.preprocess_data():\n",
    "            print(\"ERROR: Data preprocessing failed. Analysis aborted.\")\n",
    "            return False\n",
    "            \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress.update(1)\n",
    "            workflow_progress.set_description(f\"Completed: {pipeline_steps[1]}\")\n",
    "        \n",
    "        # 3. Perform statistical analysis\n",
    "        result = self.perform_statistical_analysis()\n",
    "        if not result:\n",
    "            print(\"WARNING: Statistical analysis encountered issues. Continuing with analysis.\")\n",
    "            \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress.update(1)\n",
    "            workflow_progress.set_description(f\"Completed: {pipeline_steps[2]}\")\n",
    "        \n",
    "        # 4. Perform feature selection\n",
    "        result = self.perform_feature_selection()\n",
    "        if not result:\n",
    "            print(\"WARNING: Feature selection encountered issues. Continuing with analysis.\")\n",
    "            \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress.update(1)\n",
    "            workflow_progress.set_description(f\"Completed: {pipeline_steps[3]}\")\n",
    "        \n",
    "        # 5. Perform dimensionality reduction\n",
    "        result = self.perform_dimensionality_reduction()\n",
    "        if not result:\n",
    "            print(\"WARNING: Dimensionality reduction encountered issues. Continuing with analysis.\")\n",
    "            \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress.update(1)\n",
    "            workflow_progress.set_description(f\"Completed: {pipeline_steps[4]}\")\n",
    "        \n",
    "        # 6. Perform clustering\n",
    "        if not self.perform_clustering():\n",
    "            print(\"ERROR: Clustering failed. Analysis aborted.\")\n",
    "            if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "                workflow_progress.close()\n",
    "            return False\n",
    "            \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress.update(1)\n",
    "            workflow_progress.set_description(f\"Completed: {pipeline_steps[5]}\")\n",
    "        \n",
    "        # 7. Analyze feature-cluster relationships\n",
    "        result = self.analyze_feature_clusters()\n",
    "        if not result:\n",
    "            print(\"WARNING: Feature-cluster analysis encountered issues. Continuing with analysis.\")\n",
    "            \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress.update(1)\n",
    "            workflow_progress.set_description(f\"Completed: {pipeline_steps[6]}\")\n",
    "        \n",
    "        # 8. Find doppelgangers\n",
    "        result = self.find_doppelgangers()\n",
    "        if not result:\n",
    "            print(\"WARNING: Doppelganger identification encountered issues. Continuing with analysis.\")\n",
    "            \n",
    "        if TQDM_AVAILABLE and USE_PROGRESS_BARS:\n",
    "            workflow_progress.update(1)\n",
    "            workflow_progress.set_description(f\"Completed: {pipeline_steps[7]}\")\n",
    "            workflow_progress.close()\n",
    "        \n",
    "        # Calculate total runtime\n",
    "        end_time: float = time()\n",
    "        \n",
    "        total_runtime: float = end_time - start_time\n",
    "        \n",
    "        runtime_minutes: float = total_runtime / 60\n",
    "        \n",
    "        # Save analysis summary\n",
    "        summary: Dict[str, Any] = {\n",
    "            \n",
    "            \"total_runtime_seconds\": total_runtime,\n",
    "            \n",
    "            \"total_runtime_minutes\": runtime_minutes,\n",
    "            \n",
    "            \"num_input_features\": len(self.data.columns) if self.data is not None else 0,\n",
    "            \n",
    "            \"num_selected_features\": len(self.numeric_data.columns) if self.numeric_data is not None else 0,\n",
    "            \n",
    "            \"num_observations\": len(self.data) if self.data is not None else 0,\n",
    "            \n",
    "            \"num_clusters\": len(np.unique(self.cluster_labels)) if self.cluster_labels is not None else 0,\n",
    "            \n",
    "            \"model_performance\": self.evaluation_metrics\n",
    "        \n",
    "        }\n",
    "        \n",
    "        # Save summary to file\n",
    "        summary_df = pd.DataFrame([summary])\n",
    "        \n",
    "        summary_df.to_csv(os.path.join(self.output_path, \"analysis_summary.csv\"), index=False)\n",
    "        \n",
    "        print(f\"\\nAnalysis complete in {runtime_minutes:.2f} minutes!\")\n",
    "        \n",
    "        print(f\"Results saved to: {self.output_path}\")\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"Main function to run the tract segmentation analysis.\n",
    "    \n",
    "    This function creates an instance of the TractSegmentationAnalysis class \n",
    "    and executes the full analysis pipeline. It will use GPU acceleration if available.\n",
    "    \"\"\"\n",
    "    # Print header\n",
    "    print(\"Census Tract Segmentation Analysis\")\n",
    "    \n",
    "    print(\"===================================\")\n",
    "    \n",
    "    # Check CUDA availability before creating the analyzer\n",
    "    # This was already done during module initialization but we log it again here\n",
    "    print(f\"GPU Status: {'Available with CUDA' if CUDA_AVAILABLE else 'Not available or CUDA not detected'}\")\n",
    "    \n",
    "    # Create analyzer with GPU enabled if available\n",
    "    analyzer = TractSegmentationAnalysis(use_gpu=True)  # Will fall back to CPU if GPU not available\n",
    "    \n",
    "    success = analyzer.run_full_analysis()\n",
    "    \n",
    "    # Return appropriate exit code\n",
    "    if not success:\n",
    "        \n",
    "        print(\"\\nAnalysis completed with errors.\")\n",
    "        \n",
    "        import sys\n",
    "        \n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3e084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
